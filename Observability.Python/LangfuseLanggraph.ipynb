{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be858bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "from typing import Annotated\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    " \n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    " \n",
    "graph_builder = StateGraph(State)\n",
    " \n",
    "llm = AzureChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a43146c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse import Langfuse\n",
    "\n",
    "langfuse = Langfuse(\n",
    "  secret_key=os.getenv(\"LANGFUSE_SECRET_KEY\"),\n",
    "  public_key=os.getenv(\"LANGFUSE_PUBLIC_KEY\"),\n",
    "  host=os.getenv(\"LANGFUSE_HOST\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be92f23b",
   "metadata": {},
   "source": [
    "# Simple Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd79d1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def chatbot(state: State):\n",
    "    return {\"messages\": [llm.invoke(state[\"messages\"])]}\n",
    " \n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "graph_builder.set_entry_point(\"chatbot\")\n",
    "graph_builder.set_finish_point(\"chatbot\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76e62b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chatbot': {'messages': [AIMessage(content='Langfuse is an observability and monitoring platform specifically designed for AI systems and applications. It integrates with AI models to provide insights into their performance, usage, and overall health. By offering tools for logging, tracing, error tracking, and analytics, Langfuse helps developers and operators understand how their AI systems are functioning in real-time. This enables improved debugging, optimization, and reliability of AI applications, ensuring they perform as expected and adapt to changing conditions or inputs.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 95, 'prompt_tokens': 13, 'total_tokens': 108, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_ee1d74bde0', 'id': 'chatcmpl-BJmeAXxjpLGVKRhT363PeUvMyZNse', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run-0c007642-8561-4b66-83e5-1d34e6e6eb39-0', usage_metadata={'input_tokens': 13, 'output_tokens': 95, 'total_tokens': 108, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}}\n"
     ]
    }
   ],
   "source": [
    "from langfuse.callback import CallbackHandler\n",
    "\n",
    "langfuse_handler = CallbackHandler()\n",
    " \n",
    "for s in graph.stream({\"messages\": [HumanMessage(content = \"What is Langfuse?\")]}, config={\"callbacks\": [langfuse_handler]}):\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f48a58",
   "metadata": {},
   "source": [
    "# Multi Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d97535ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    " \n",
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "from datetime import datetime\n",
    "from langchain.tools import Tool\n",
    " \n",
    "# Define a tools that searches Wikipedia\n",
    "wikipedia_tool = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
    " \n",
    "# Define a new tool that returns the current datetime\n",
    "datetime_tool = Tool(\n",
    "    name=\"Datetime\",\n",
    "    func = lambda x: datetime.now().isoformat(),\n",
    "    description=\"Returns the current datetime\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "986860f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    " \n",
    "def create_agent(llm: AzureChatOpenAI, system_prompt: str, tools: list):\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                system_prompt,\n",
    "            ),\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "            MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "        ]\n",
    "    )\n",
    "    agent = create_openai_tools_agent(llm, tools, prompt)\n",
    "    executor = AgentExecutor(agent=agent, tools=tools)\n",
    "    return executor\n",
    " \n",
    "def agent_node(state, agent, name):\n",
    "    result = agent.invoke(state)\n",
    "    return {\"messages\": [HumanMessage(content=result[\"output\"], name=name)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bea77cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\derek.arends\\AppData\\Local\\Temp\\ipykernel_22948\\4181291937.py:49: LangChainDeprecationWarning: The method `BaseChatOpenAI.bind_functions` was deprecated in langchain-openai 0.2.1 and will be removed in 1.0.0. Use :meth:`~langchain_openai.chat_models.base.ChatOpenAI.bind_tools` instead.\n",
      "  | llm.bind_functions(functions=[function_def], function_call=\"route\")\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    " \n",
    "members = [\"Researcher\", \"CurrentTime\"]\n",
    "system_prompt = (\n",
    "    \"You are a supervisor tasked with managing a conversation between the\"\n",
    "    \" following workers:  {members}. Given the following user request,\"\n",
    "    \" respond with the worker to act next. Each worker will perform a\"\n",
    "    \" task and respond with their results and status. When finished,\"\n",
    "    \" respond with FINISH.\"\n",
    ")\n",
    "# Our team supervisor is an LLM node. It just picks the next agent to process and decides when the work is completed\n",
    "options = [\"FINISH\"] + members\n",
    " \n",
    "# Using openai function calling can make output parsing easier for us\n",
    "function_def = {\n",
    "    \"name\": \"route\",\n",
    "    \"description\": \"Select the next role.\",\n",
    "    \"parameters\": {\n",
    "        \"title\": \"routeSchema\",\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"next\": {\n",
    "                \"title\": \"Next\",\n",
    "                \"anyOf\": [\n",
    "                    {\"enum\": options},\n",
    "                ],\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"next\"],\n",
    "    },\n",
    "}\n",
    " \n",
    "# Create the prompt using ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Given the conversation above, who should act next?\"\n",
    "            \" Or should we FINISH? Select one of: {options}\",\n",
    "        ),\n",
    "    ]\n",
    ").partial(options=str(options), members=\", \".join(members))\n",
    " \n",
    "supervisor_chain = (\n",
    "    prompt\n",
    "    | llm.bind_functions(functions=[function_def], function_call=\"route\")\n",
    "    | JsonOutputFunctionsParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6c2c4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import operator\n",
    "from typing import Sequence, TypedDict\n",
    "from langgraph.graph import END, StateGraph, START\n",
    " \n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    next: str\n",
    " \n",
    "research_agent = create_agent(llm, \"You are a web researcher.\", [wikipedia_tool])\n",
    "research_node = functools.partial(agent_node, agent=research_agent, name=\"Researcher\")\n",
    " \n",
    "currenttime_agent = create_agent(llm, \"You can tell the current time at\", [datetime_tool])\n",
    "currenttime_node = functools.partial(agent_node, agent=currenttime_agent, name = \"CurrentTime\")\n",
    " \n",
    "workflow = StateGraph(AgentState)\n",
    " \n",
    "workflow.add_node(\"Researcher\", research_node)\n",
    "workflow.add_node(\"CurrentTime\", currenttime_node)\n",
    "workflow.add_node(\"supervisor\", supervisor_chain)\n",
    " \n",
    "# We want our workers to ALWAYS \"report back\" to the supervisor when done\n",
    "for member in members:\n",
    "    workflow.add_edge(member, \"supervisor\")\n",
    " \n",
    "conditional_map = {k: k for k in members}\n",
    "conditional_map[\"FINISH\"] = END\n",
    "workflow.add_conditional_edges(\"supervisor\", lambda x: x[\"next\"], conditional_map)\n",
    "workflow.add_edge(START, \"supervisor\")\n",
    " \n",
    "graph_2 = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2bdec7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'supervisor': {'next': 'CurrentTime'}}\n",
      "----\n",
      "{'CurrentTime': {'messages': [HumanMessage(content='As of April 7, 2025, the current President of the United States is Joe Biden. However, to ensure up-to-date information, you might want to check a reliable source or the latest news.', additional_kwargs={}, response_metadata={}, name='CurrentTime')]}}\n",
      "----\n",
      "{'supervisor': {'next': 'Researcher'}}\n",
      "----\n",
      "{'Researcher': {'messages': [HumanMessage(content='As of April 7, 2025, the current President of the United States is Donald Trump, who assumed office on January 20, 2025.', additional_kwargs={}, response_metadata={}, name='Researcher')]}}\n",
      "----\n",
      "{'supervisor': {'next': 'FINISH'}}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "from langfuse.callback import CallbackHandler\n",
    " \n",
    "# Initialize Langfuse CallbackHandler for Langchain (tracing)\n",
    "langfuse_handler = CallbackHandler()\n",
    " \n",
    "# Add Langfuse handler as callback: config={\"callbacks\": [langfuse_handler]}\n",
    "# You can also set an optional 'run_name' that will be used as the trace name in Langfuse\n",
    "for s in graph_2.stream({\"messages\": [HumanMessage(content = \"Who is the current president of the USA? Make sure to check the date and search wiki\")]},\n",
    "                      config={\"callbacks\": [langfuse_handler]}):\n",
    "    print(s)\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07a4ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(graph_2.get_graph().draw_mermaid_png()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
